{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "89993862-a40d-4f15-81a9-7cb8698e14a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import seaborn as sns\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from IPython import display\n",
    "import imageio\n",
    "import os\n",
    "FP = './animations'\n",
    "# !pipreqsnb --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "e75f6899-a03f-467e-a05a-09a6ac0834b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aux_func import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "02786d9c-b926-4487-991f-dd0262b63552",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleDiscretized:\n",
    "    \"\"\"overrides most of that *** **** ****, \n",
    "        NB! not everything\"\"\"\n",
    "    from types import SimpleNamespace\n",
    "    from functools import wraps\n",
    "    \n",
    "    def discrete(method):\n",
    "        \"\"\"overwrites method's output state (considered to be the first element output in returned tuple)\"\"\"\n",
    "        def inner(*args, **kwargs):\n",
    "            results = method(*args, **kwargs)\n",
    "            print(results[0])\n",
    "            return list([args[0].encode(results[0]), *results[1:]])\n",
    "        return inner\n",
    "    \n",
    "    def __init__(self, aid_to_str=ACTIONS_STR, discr_cfg=(8, 5, 8, 5)):\n",
    "        self.e = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "        self.aid_to_str = ACTIONS_STR = ('<','>') # for policy visualization\n",
    "        \n",
    "        # i do hate gym environment interface for this particular thing\n",
    "        self.action_space = SimpleNamespace(n=self.e.action_space.n)\n",
    "        self.unwrapped = SimpleNamespace(spec=SimpleNamespace(id='CartPoleDiscretized-v1'))\n",
    "        self.n_states = np.prod(discr_cfg)\n",
    "        self.n_actions = self.action_space.n\n",
    "        self.observation_space = SimpleNamespace(n=self.n_states)\n",
    "        \n",
    "        self.sd = self.e.observation_space.shape[0]\n",
    "        self.ul = self.e.observation_space.high\n",
    "        self.ll = self.e.observation_space.low\n",
    "        self.b_eds, self.b_dims, self.b_rdms = self.get_bins(discr_cfg)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Discretized {self.e.unwrapped.spec.id} environment with {self.b_dims}={self.observation_space.n} states and {self.action_space.n} actions {self.aid_to_str}, named as {self.unwrapped.spec.id}\"\n",
    "        \n",
    "    def get_bins(self, config):\n",
    "        \"\"\"handles list/tuple of both bin sizes or 2d array with exact bin values\"\"\"\n",
    "        bin_edges, bin_sizes, raw_bin_sizes = [], [], []\n",
    "        for i, b in enumerate(config):\n",
    "            if isinstance(b, (int, np.integer)):\n",
    "                bin_sizes.append(b)\n",
    "                raw_bin_sizes.append(b-1)\n",
    "                bin_edges.append(np.linspace(self.ll[i]/2, self.ul[i]/2, num=b))\n",
    "            else:\n",
    "                bin_sizes.append(len(b))\n",
    "                raw_bin_sizes.append(len(b)-1)\n",
    "                bin_edges.append(b)\n",
    "        return bin_edges, bin_sizes, raw_bin_sizes\n",
    "        \n",
    "    def discretize(self, observation):\n",
    "        \"\"\"multidimensional discretization of 4-tuple of floats, s.that each bin is defined by its edge, i.e. is a half-open interval [b_va[i-1], b_va[i])\"\"\"\n",
    "        return list(np.digitize(o, b) for o, b in zip(observation, self.b_eds))\n",
    "\n",
    "    def encode(self, observation):\n",
    "        \"\"\"represents 4-tuple of discretized observation as a single integer\"\"\"\n",
    "        print(self.b_dims, self.discretize(observation))\n",
    "        return np.ravel_multi_index(self.discretize(observation), dims=self.b_dims).item()\n",
    "        \n",
    "    @discrete\n",
    "    def reset(self):\n",
    "        return self.e.reset()\n",
    "        \n",
    "    @discrete\n",
    "    def step(self, action):\n",
    "        return self.e.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "62ff8c9b-e401-449d-b686-b16a5b6b3a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discretized CartPole-v1 environment with [7, 4, 7, 4]=1600 states and 2 actions ('<', '>'), named as CartPoleDiscretized-v1"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENV = CartPoleDiscretized()\n",
    "ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "6f78e42c-ec94-407d-8f40-b50543120ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelFreeAgent(RandAgent):\n",
    "    \"\"\"Model-free agent == R, TP functions of ENV aren't provided and aren't (explicitly?) approximated\"\"\"\n",
    "    def __init__(self, env, aid_to_str):\n",
    "        super().__init__(env=env, aid_to_str=aid_to_str)\n",
    "        self.stc = 0 # counts improvement steps \n",
    "\n",
    "    def gpi(self, q, eps, zero_mask=None):\n",
    "        \"\"\"epsilon-greedy policy improvement based on values of q(s,a), at each state,\n",
    "        policy ~ not only take an action that maximizes given action-value function q, \n",
    "        but also allow \"the rest' actions happen with eps probability (shared)\n",
    "        \n",
    "        unvisited states have q=0 but that doesn't matter anything, \n",
    "        as rewards are < 0, such states would impact argmax in vain,\n",
    "        zero_mask removes them from computation\"\"\"\n",
    "        if zero_mask is not None:\n",
    "            q[zero_mask] = np.full(shape=q.shape, fill_value=-10)[zero_mask]\n",
    "            # q = np.ma.array(q, mask=zero_mask)\n",
    "        # epsilon-greedy: let eps be ~\"the rest' actions probability, split it between the all actions\n",
    "        dummy = (1 - eps + eps / self.n_actions) * np.eye(self.n_actions) + (eps / self.n_actions) * (np.ones(self.n_actions) - np.eye(self.n_actions))\n",
    "        best_actions = np.argmax(q, axis=0) # 1D array of 'best' actions\n",
    "        # one-hot encoding of best_actions array (used to choose rows from dummy)\n",
    "        self.policy = dummy[best_actions]\n",
    "        self.stc += 1\n",
    "\n",
    "    def fit(self, n_trajectories, max_length, alpha_d=0.5, gamma=0.99, eps_d=None, verbose=False):\n",
    "        dh = display.display(display_id=True)\n",
    "        eps_rates, alpha_rates = [], []\n",
    "        if callable(alpha_d):\n",
    "            alpha_d.n_total = n_trajectories\n",
    "        if eps_d is not None:\n",
    "            eps_d.n_total = n_trajectories\n",
    "        else:\n",
    "            eps_d = LinearAR(n_iterations=n_trajectories, start=1)\n",
    "        for i, t in enumerate(range(n_trajectories)):\n",
    "            # decrease eps throughout the loop\n",
    "            eps = eps_d(i)\n",
    "            # decrease alpha ~ learning rate\n",
    "            alpha = alpha_d(i) if callable(alpha_d) else alpha_d\n",
    "            # policy evaluation and improvement throughout trajectory\n",
    "            results = self.walk(max_length=max_length, alpha=alpha, eps=eps, gamma=gamma)\n",
    "            # visualization\n",
    "            eps_rates.append(eps)\n",
    "            alpha_rates.append(alpha)\n",
    "            self.log.append(sum(results['r']))\n",
    "            if verbose:\n",
    "                # ax = self.show_policy()\n",
    "                ax = self.learning_curve(title=f\"Total reward at trajectory of length < {max_length}\", l_scale=True)\n",
    "                ax2 = ax.twinx()\n",
    "                ax2.tick_params(axis='y', labelcolor='blueviolet')\n",
    "                ax2 = sns.lineplot(eps_rates, linewidth=0.5, ax=ax2, label=\"exploration, ε‎\", color='blueviolet')\n",
    "                sns.lineplot(alpha_rates, linewidth=0.5, ax=ax2, label=\"learning rate, α\", color='orchid')\n",
    "                dh.update(plt.gcf())\n",
    "                plt.close() # because plt.clf() is spurious\n",
    "\n",
    "class SARSAAgent(ModelFreeAgent):\n",
    "    \"\"\"SARSA updates at each step of every trajectory\"\"\"\n",
    "    def __init__(self, env, aid_to_str):\n",
    "        super().__init__(env=env, aid_to_str=aid_to_str)\n",
    "        self.Q = np.zeros_like(self.policy.T)\n",
    "        self.fly = self.sarsa_step\n",
    "\n",
    "    def sarsa_step(self, queue_6, alpha, eps, gamma):\n",
    "        \"\"\"accepts a queue object, updates value-function q and policy in sarsa manner\"\"\"\n",
    "        if len(queue_6) == 6: # sarsar\n",
    "            s, a, r, sx, ax = list(queue_6)[:5]\n",
    "            self.Q[a, s] += alpha * (r + gamma * self.Q[ax, sx] - self.Q[a, s])\n",
    "            self.gpi(self.Q, eps)\n",
    "\n",
    "class QLearningAgent(ModelFreeAgent):\n",
    "    \"\"\"Q updates at each step of every trajectory\"\"\"\n",
    "    def __init__(self, env, aid_to_str):\n",
    "        super().__init__(env=env, aid_to_str=aid_to_str)\n",
    "        self.Q = np.zeros_like(self.policy.T)\n",
    "        self.fly = self.ql_step\n",
    "\n",
    "    def ql_step(self, queue_6, alpha, eps, gamma):\n",
    "        \"\"\"accepts a queue object, updates value-function q and policy ~ Q-learning\"\"\"\n",
    "        if len(queue_6) == 6: # sarsar\n",
    "            s, a, r, sx = list(queue_6)[:4]\n",
    "            self.Q[a, s] += alpha * (r + gamma * np.max(self.Q[:, sx]) - self.Q[a, s])\n",
    "            self.gpi(self.Q, eps)\n",
    "\n",
    "class MonteCarloAgent(ModelFreeAgent):\n",
    "    \"\"\"Applies Monte-Carlo sampling of trajectories for action-value function q approximation\"\"\"\n",
    "    def __init__(self, env, aid_to_str):\n",
    "        super().__init__(env=env, aid_to_str=aid_to_str)\n",
    "    \n",
    "    def walk_r(self, gamma, max_length):\n",
    "        \"\"\"transforms standard results after tracing a route, yields\n",
    "            n (matrix a,s of visited state counts), \n",
    "            g (matrix a,s of returns)\"\"\"\n",
    "        g, n = np.zeros_like(self.policy.T), np.zeros_like(self.policy.T)\n",
    "        trajectory = super().walk(max_length=max_length)\n",
    "        R, A, S = trajectory['r'], trajectory['a'], trajectory['s']\n",
    "        # get rewards serie (summands, w/ discounting)\n",
    "        gamma_ = np.cumprod(np.concatenate((np.atleast_1d(1.), np.tile(np.array(gamma), len(R) - 1))))\n",
    "        R_discounted = gamma_ * np.array(R)\n",
    "        # get values of returns G, e.g. total reward as if we'd started at timestep t and continued MDP on this trajectory\n",
    "        G = np.array([np.sum(R_discounted[t:]) / gamma**t for t in range(len(R_discounted))])\n",
    "        # get unique pairs from 2D array, their places and counts\n",
    "        unp, pos, cts = np.unique(np.vstack([A, S]), axis=1, return_inverse=True, return_counts=True)\n",
    "        # row vector of stacked Gt values @ matrix with (stacked as rows ~ pos) one-hot encoded unique states => row vector with sum of Gts at unique states\n",
    "        g[*unp], n[*unp] = G @ (np.eye(len(cts))[pos]), cts\n",
    "        return g, n, sum(R)\n",
    "\n",
    "    def fit(self, n_trajectories, max_length, eps_d=None, gamma=1, verbose=False):\n",
    "        \"\"\"MC approach replaces expected value calculation with an empirical mean of return Gt\"\"\"\n",
    "        dh = display.display(display_id=True)\n",
    "        if eps_d is not None:\n",
    "            eps_d.n_total = n_trajectories\n",
    "        else:\n",
    "            eps_d = LinearAR(n_iterations=n_trajectories, start=1)      \n",
    "        eps_rates = []\n",
    "        # np.full(shape=self.policy.T.shape, fill_value=-np.inf)\n",
    "        G, N = np.zeros_like(self.policy.T), np.zeros_like(self.policy.T)\n",
    "        for i, t in enumerate(range(n_trajectories)):\n",
    "            # MC policy evaluation to approximate Q\n",
    "            g, n, r = self.walk_r(max_length=max_length, gamma=gamma)\n",
    "            G, N = G + g, N + n\n",
    "            Q = G / (N_ := N.copy() + 1E-2 * (N == 0)) # prevent zero-division error\n",
    "            \n",
    "            # decrease eps throughout the loop\n",
    "            eps = eps_d(i)\n",
    "            \n",
    "            # policy improvement (non-visited sa pairs are treated different)\n",
    "            self.gpi(Q, eps=eps, zero_mask=N==0)\n",
    "\n",
    "            # visualization\n",
    "            self.log.append(r)\n",
    "            eps_rates.append(eps)\n",
    "            if verbose:\n",
    "                # ax = self.show_policy()\n",
    "                ax = self.learning_curve(title=f\"Total reward at trajectory of length < {max_length}\", l_scale=True)\n",
    "                ax2 = ax.twinx()\n",
    "                ax2.tick_params(axis='y', labelcolor='blueviolet')\n",
    "                sns.lineplot(eps_rates, linewidth=0.5, ax=ax2, label=\"exploration, ε\", color='blueviolet')\n",
    "                dh.update(plt.gcf())\n",
    "                plt.close() # because plt.clf() is spurious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "9aa8d40d-df5f-4998-8e6f-834355ed52e4",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02209247 -0.03849752 -0.03667387  0.02134334]\n",
      "[7, 4, 7, 4] [4, 2, 3, 3]\n",
      "[ 0.02132251 -0.23307486 -0.036247    0.3022334 ]\n",
      "[7, 4, 7, 4] [4, 2, 3, 3]\n",
      "[ 0.01666102 -0.42766196 -0.03020233  0.58326805]\n",
      "[7, 4, 7, 4] [4, 2, 3, 3]\n",
      "[ 0.00810778 -0.2321302  -0.01853697  0.28122604]\n",
      "[7, 4, 7, 4] [4, 2, 4, 3]\n",
      "[ 0.00346517 -0.03674881 -0.01291245 -0.01724527]\n",
      "[7, 4, 7, 4] [4, 2, 4, 2]\n",
      "[ 0.0027302  -0.23168322 -0.01325736  0.2713358 ]\n",
      "[7, 4, 7, 4] [4, 2, 4, 3]\n",
      "[-0.00190347 -0.03637462 -0.00783064 -0.02549888]\n",
      "[7, 4, 7, 4] [4, 2, 4, 2]\n",
      "[-0.00263096  0.15885875 -0.00834062 -0.32064214]\n",
      "[7, 4, 7, 4] [4, 3, 4, 2]\n",
      "[ 5.4621627e-04  3.5409847e-01 -1.4753459e-02 -6.1594367e-01]\n",
      "[7, 4, 7, 4] [4, 3, 4, 2]\n",
      "[ 0.00762819  0.15918572 -0.02707233 -0.3279437 ]\n",
      "[7, 4, 7, 4] [4, 3, 4, 2]\n",
      "[ 0.0108119  -0.03554056 -0.03363121 -0.04391961]\n",
      "[7, 4, 7, 4] [4, 2, 3, 2]\n",
      "[ 0.01010109  0.1600471  -0.0345096  -0.34702092]\n",
      "[7, 4, 7, 4] [4, 3, 3, 2]\n",
      "[ 0.01330203  0.3556425  -0.04145002 -0.6503831 ]\n",
      "[7, 4, 7, 4] [4, 3, 3, 2]\n",
      "[ 0.02041488  0.5513165  -0.05445768 -0.9558249 ]\n",
      "[7, 4, 7, 4] [4, 3, 3, 2]\n",
      "[ 0.03144121  0.35696766 -0.07357418 -0.68073636]\n",
      "[7, 4, 7, 4] [4, 3, 3, 2]\n",
      "[ 0.03858056  0.5530302  -0.08718891 -0.9956459 ]\n",
      "[7, 4, 7, 4] [4, 3, 3, 2]\n",
      "[ 0.04964117  0.35917568 -0.10710182 -0.7315697 ]\n",
      "[7, 4, 7, 4] [4, 3, 2, 2]\n",
      "[ 0.05682468  0.16568398 -0.12173322 -0.474423  ]\n",
      "[7, 4, 7, 4] [4, 3, 2, 2]\n",
      "[ 0.06013836 -0.02752753 -0.13122168 -0.2224517 ]\n",
      "[7, 4, 7, 4] [4, 2, 2, 2]\n",
      "[ 0.05958781 -0.2205536  -0.1356707   0.02612963]\n",
      "[7, 4, 7, 4] [4, 2, 2, 3]\n",
      "[ 0.05517674 -0.02377322 -0.13514812 -0.3060935 ]\n",
      "[7, 4, 7, 4] [4, 2, 2, 2]\n",
      "[ 0.05470128 -0.21673647 -0.14126998 -0.05890069]\n",
      "[7, 4, 7, 4] [4, 2, 2, 2]\n",
      "[ 0.05036655 -0.40958    -0.14244801  0.18608755]\n",
      "[7, 4, 7, 4] [4, 2, 2, 3]\n",
      "[ 0.04217495 -0.602407   -0.13872625  0.43066022]\n",
      "[7, 4, 7, 4] [4, 2, 2, 3]\n",
      "[ 0.03012681 -0.7953199  -0.13011305  0.67659485]\n",
      "[7, 4, 7, 4] [4, 2, 2, 3]\n",
      "[ 0.01422041 -0.98841697 -0.11658115  0.92564666]\n",
      "[7, 4, 7, 4] [4, 2, 2, 3]\n",
      "[-0.00554793 -1.1817878  -0.09806821  1.1795369 ]\n",
      "[7, 4, 7, 4] [4, 2, 2, 3]\n",
      "[-0.02918369 -0.985539   -0.07447748  0.8577922 ]\n",
      "[7, 4, 7, 4] [4, 2, 3, 3]\n",
      "[-0.04889447 -0.78948575 -0.05732163  0.54265136]\n",
      "[7, 4, 7, 4] [4, 2, 3, 3]\n",
      "[-0.06468418 -0.9837572  -0.04646861  0.81673664]\n",
      "[7, 4, 7, 4] [4, 2, 3, 3]\n",
      "[-0.08435933 -0.7880309  -0.03013388  0.5098073 ]\n",
      "[7, 4, 7, 4] [4, 2, 3, 3]\n",
      "[-0.10011995 -0.5924977  -0.01993773  0.20778258]\n",
      "[7, 4, 7, 4] [4, 2, 4, 3]\n",
      "[-0.1119699  -0.39709643 -0.01578208 -0.0911224 ]\n",
      "[7, 4, 7, 4] [4, 2, 4, 2]\n",
      "[-0.11991183 -0.20175186 -0.01760453 -0.3887425 ]\n",
      "[7, 4, 7, 4] [4, 2, 4, 2]\n",
      "[-0.12394687 -0.00638451 -0.02537937 -0.68692356]\n",
      "[7, 4, 7, 4] [4, 2, 4, 2]\n",
      "[-0.12407456 -0.20114514 -0.03911785 -0.40233743]\n",
      "[7, 4, 7, 4] [4, 2, 3, 2]\n",
      "[-0.12809746 -0.00549083 -0.0471646  -0.7070921 ]\n",
      "[7, 4, 7, 4] [4, 2, 3, 2]\n",
      "[-0.12820728 -0.19992875 -0.06130644 -0.42962083]\n",
      "[7, 4, 7, 4] [4, 2, 3, 2]\n",
      "[-0.13220584 -0.00399462 -0.06989885 -0.74098265]\n",
      "[7, 4, 7, 4] [4, 2, 3, 2]\n",
      "[-0.13228574 -0.19808552 -0.08471851 -0.4710908 ]\n",
      "[7, 4, 7, 4] [4, 2, 3, 2]\n",
      "[-0.13624746 -0.00187551 -0.09414032 -0.7892277 ]\n",
      "[7, 4, 7, 4] [4, 2, 2, 2]\n",
      "[-0.13628496 -0.19558719 -0.10992488 -0.5275832 ]\n",
      "[7, 4, 7, 4] [4, 2, 2, 2]\n",
      "[-0.14019671 -0.3890048  -0.12047654 -0.27146077]\n",
      "[7, 4, 7, 4] [4, 2, 2, 2]\n",
      "[-0.1479768  -0.58222    -0.12590575 -0.01907318]\n",
      "[7, 4, 7, 4] [4, 2, 2, 2]\n",
      "[-0.15962121 -0.3855384  -0.12628722 -0.3486789 ]\n",
      "[7, 4, 7, 4] [4, 2, 2, 2]\n",
      "[-0.16733198 -0.5786592  -0.1332608  -0.09833448]\n",
      "[7, 4, 7, 4] [4, 2, 2, 2]\n",
      "[-0.17890516 -0.7716445  -0.13522749  0.14951414]\n",
      "[7, 4, 7, 4] [4, 2, 2, 3]\n",
      "[-0.19433805 -0.964597   -0.13223721  0.39666474]\n",
      "[7, 4, 7, 4] [4, 2, 2, 3]\n",
      "[-0.21362999 -0.7678711  -0.12430391  0.06538767]\n",
      "[7, 4, 7, 4] [4, 2, 2, 3]\n",
      "[-0.22898741 -0.57120645 -0.12299616 -0.26378444]\n",
      "[7, 4, 7, 4] [4, 2, 2, 2]\n",
      "[-0.24041153 -0.7643778  -0.12827185 -0.0122862 ]\n",
      "[7, 4, 7, 4] [4, 2, 2, 2]\n",
      "[-0.2556991  -0.9574491  -0.12851757  0.23733288]\n",
      "[7, 4, 7, 4] [4, 2, 2, 3]\n",
      "[-0.27484807 -1.1505231  -0.12377091  0.48687527]\n",
      "[7, 4, 7, 4] [4, 2, 2, 3]\n",
      "[-0.29785854 -1.3437012  -0.11403341  0.7381299 ]\n",
      "[7, 4, 7, 4] [4, 2, 2, 3]\n",
      "[-0.32473257 -1.1472046  -0.09927081  0.41184613]\n",
      "[7, 4, 7, 4] [4, 2, 2, 3]\n",
      "[-0.34767666 -1.3407894  -0.09103388  0.67165613]\n",
      "[7, 4, 7, 4] [3, 2, 2, 3]\n",
      "[-0.37449244 -1.5345359  -0.07760076  0.93434536]\n",
      "[7, 4, 7, 4] [3, 2, 3, 3]\n",
      "[-0.40518317 -1.7285302  -0.05891386  1.2016692 ]\n",
      "[7, 4, 7, 4] [3, 2, 3, 3]\n",
      "[-0.43975377 -1.5326979  -0.03488047  0.89111984]\n",
      "[7, 4, 7, 4] [3, 2, 3, 3]\n",
      "[-0.47040772 -1.3371205  -0.01705807  0.58767945]\n",
      "[7, 4, 7, 4] [3, 2, 4, 3]\n",
      "[-0.49715012 -1.5319995  -0.00530449  0.8749405 ]\n",
      "[7, 4, 7, 4] [3, 2, 4, 3]\n",
      "[-0.5277901  -1.7270489   0.01219433  1.165951  ]\n",
      "[7, 4, 7, 4] [3, 2, 4, 3]\n",
      "[-0.5623311  -1.9223274   0.03551335  1.4624321 ]\n",
      "[7, 4, 7, 4] [3, 2, 5, 3]\n",
      "[-0.6007776  -1.7276583   0.06476199  1.1810511 ]\n",
      "[7, 4, 7, 4] [3, 2, 5, 3]\n",
      "[-0.6353308  -1.9235582   0.08838301  1.4933119 ]\n",
      "[7, 4, 7, 4] [3, 2, 5, 3]\n",
      "[-0.67380196 -2.1196373   0.11824925  1.8122333 ]\n",
      "[7, 4, 7, 4] [3, 2, 6, 3]\n",
      "[-0.71619475 -1.9260147   0.15449391  1.5585119 ]\n",
      "[7, 4, 7, 4] [3, 2, 7, 3]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid entry in coordinates array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[466], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m ann_sarsa_a \u001b[38;5;241m=\u001b[39m LinearAR(start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m      3\u001b[0m ann_sarsa_e \u001b[38;5;241m=\u001b[39m ExponentialAR(la\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-1\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43msarsa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_trajectories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m950\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mann_sarsa_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps_d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mann_sarsa_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.999\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[465], line 40\u001b[0m, in \u001b[0;36mModelFreeAgent.fit\u001b[1;34m(self, n_trajectories, max_length, alpha_d, gamma, eps_d, verbose)\u001b[0m\n\u001b[0;32m     38\u001b[0m alpha \u001b[38;5;241m=\u001b[39m alpha_d(i) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(alpha_d) \u001b[38;5;28;01melse\u001b[39;00m alpha_d\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# policy evaluation and improvement throughout trajectory\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwalk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# visualization\u001b[39;00m\n\u001b[0;32m     42\u001b[0m eps_rates\u001b[38;5;241m.\u001b[39mappend(eps)\n",
      "File \u001b[1;32m~\\PycharmProjects\\DRL_1\\aux_func.py:57\u001b[0m, in \u001b[0;36mRandAgent.walk\u001b[1;34m(self, max_length, render, **interkwargs)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_length):\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# perform an action\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(state)\n\u001b[1;32m---> 57\u001b[0m     new_state, reward, done \u001b[38;5;241m=\u001b[39m \u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# log\u001b[39;00m\n\u001b[0;32m     59\u001b[0m     states\u001b[38;5;241m.\u001b[39mappend(state)  \u001b[38;5;66;03m# append OLD state, everything breaks if you start from new\u001b[39;00m\n",
      "File \u001b[1;32m~\\PycharmProjects\\DRL_1\\aux_func.py:27\u001b[0m, in \u001b[0;36mact\u001b[1;34m(action, env)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mact\u001b[39m(action, env):\n\u001b[0;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"filters unnecessary output\"\"\"\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m3\u001b[39m]\n",
      "Cell \u001b[1;32mIn[463], line 12\u001b[0m, in \u001b[0;36mCartPoleDiscretized.discrete.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     10\u001b[0m results \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(results[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m([\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39mresults[\u001b[38;5;241m1\u001b[39m:]])\n",
      "Cell \u001b[1;32mIn[463], line 53\u001b[0m, in \u001b[0;36mCartPoleDiscretized.encode\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"represents 4-tuple of discretized observation as a single integer\"\"\"\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_dims, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscretize(observation))\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel_multi_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiscretize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb_dims\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mValueError\u001b[0m: invalid entry in coordinates array"
     ]
    }
   ],
   "source": [
    "sarsa = SARSAAgent(env=ENV, aid_to_str=ENV.aid_to_str)\n",
    "ann_sarsa_a = LinearAR(start=0.5)\n",
    "ann_sarsa_e = ExponentialAR(la=1e-1)\n",
    "sarsa.fit(n_trajectories=1000, max_length=950, alpha_d=ann_sarsa_a, eps_d=ann_sarsa_e, gamma=0.999, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c6ad27-4490-41b6-93ce-d6ec0dfefdc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
