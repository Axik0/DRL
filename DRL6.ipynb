{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8793010-268a-45bf-856e-0751e32ef802",
   "metadata": {},
   "outputs": [],
   "source": [
    "## !pip3 install -q -r requirements.txt\n",
    "# !pipreqsnb --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f1a1593-34aa-4b90-9949-69c570debeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import seaborn as sns\n",
    "from IPython import display\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "from torch.distributions import Normal\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c054b98d-7cf0-4940-ace3-1581e8954043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import ModelFreeAgent, CRandAgent\n",
    "from aux_func import LinearAR, ExponentialAR, AnnealingRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9773d1da-3810-4b77-ba9c-ad43ed3e0bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# Pendulum-v1LunarLanderContinuous-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3441d062-2af1-4d85-8007-178a20ab51f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOIDataset(Dataset):\n",
    "    '''intermediate structure for PyTorch DataLoader'''\n",
    "    def __init__(self, states, actions, returns, advantage_values):\n",
    "        self.S = states\n",
    "        self.A = actions\n",
    "        self.Af = advantage_values\n",
    "        self.R = returns\n",
    "        assert len(self.S) == len(self.A) == len(self.Af) == len(self.R), 'wrong input'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.S)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.S[i], self.A[i], self.Af[i], self.R[i], i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "014c9dbf-55c1-4f14-9a78-d8f085768dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent(ModelFreeAgent, CRandAgent):\n",
    "    def __init__(self, env, aid_to_str=None, hidden_d=(64, 32), device=DEVICE):\n",
    "        super().__init__(env=env, aid_to_str=aid_to_str)\n",
    "        self.d_actions = env.unwrapped.action_space.shape[0]\n",
    "        self.a_ll = torch.Tensor(env.action_space.low)\n",
    "        self.a_ul = torch.Tensor(env.action_space.high)\n",
    "        \n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = device\n",
    "        self.frozen_pi = None\n",
    "        self.z_eps = 1E-3 # for internal zero-division issues\n",
    "        \n",
    "        # model for a value function (scalar)\n",
    "        self.V_model = nn.Sequential(\n",
    "            nn.Linear(in_features=self.d_states, out_features=hidden_d[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_d[0], out_features=hidden_d[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_d[1], 1))\n",
    "        \n",
    "        # predicts parameters mu, std of action distribution (implying that's a Normal d.)\n",
    "        self.pi_model = nn.Sequential(\n",
    "            nn.Linear(in_features=self.d_states, out_features=hidden_d[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_d[0], out_features=hidden_d[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_d[1], 2 * self.d_actions),\n",
    "            # nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # on-the-fly advantage values calculation\n",
    "        self.fly = self.adv\n",
    "        self.Af = None\n",
    "        \n",
    "    def adv(self, queue_6, terminated, last_state, gamma):\n",
    "        \"\"\"Calculates advantage function online, throughout current trajectory,\n",
    "            each tensor allows backprop gradient flow\"\"\"\n",
    "        if len(queue_6) == 6:  # sarsar\n",
    "            # cast to torch tensors on device with float dtype (evades longint inheritance)\n",
    "            s, a, r, sx, ax, rx = map(lambda x: torch.tensor(x, dtype=torch.float, device=self.device), list(queue_6))\n",
    "            self.Af.append(r + gamma * self.V_model(sx).detach() - self.V_model(s))\n",
    "            # additional xxx|sar + terminal state as next action doesn't happen\n",
    "            if terminated: # done or exhausted\n",
    "                s, a, r, sx = sx, ax, rx, torch.tensor(last_state, dtype=torch.float, device=self.device)\n",
    "                self.Af.append(r + gamma * self.V_model(sx).detach() - self.V_model(s))\n",
    "                \n",
    "\n",
    "    def pi_distr(self, states_tensor):\n",
    "        \"\"\"transforms net output of current policy model to parameters of distribution on actions at these states (as multidimensional continuous RV)\n",
    "        torch-compliant method, allows backprop gradient flow\"\"\"\n",
    "        # split net output on two parts\n",
    "        mean_raw, std_raw = torch.split(self.pi_model(states_tensor), self.d_actions, dim=-1)\n",
    "        # ensure that they do obey limits (as parameters of Normal distribution)\n",
    "        mean = torch.clamp(mean_raw, self.a_ll + self.z_eps ** 2, self.a_ul - self.z_eps ** 2) # clipping for a given action interval\n",
    "        std = nn.functional.threshold(std_raw, threshold=0, value=self.z_eps) # prevent non-positive std\n",
    "        if torch.isnan(mean_raw).any():\n",
    "            print('FUGG')\n",
    "        return Normal(mean, std)\n",
    "        \n",
    "        \n",
    "    def walk_ga(self, max_length, gamma):\n",
    "        \"\"\"transforms standard results after tracing a route, yields\n",
    "            standard SA as numpy arrays, total reward, then G-returns and Advantage value tensors\n",
    "            only Af values have .grad attribute\"\"\"\n",
    "        self.Af = []\n",
    "        trajectory = super().walk(max_length=max_length, gamma=gamma)\n",
    "        S, A, R = trajectory['s'], trajectory['a'], trajectory['r']\n",
    "        #get rewards serie (summands, w/ discounting)\n",
    "        gamma_ = np.cumprod(np.concatenate((np.atleast_1d(1.), np.tile(np.array(gamma), len(R) - 1))))\n",
    "        R_discounted = gamma_ * np.array(R)\n",
    "        G = np.array([np.sum(R_discounted[t:]) / gamma ** t for t in range(len(R_discounted))])\n",
    "        return torch.Tensor(np.array(S)), torch.Tensor(np.array(A)), torch.sum(torch.Tensor(R)).unsqueeze(0), torch.tensor(G, dtype=torch.float), torch.stack(self.Af)\n",
    "        \n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"samples an action from a normal distribution (mean, std are given by NN)\"\"\"\n",
    "        # convert to torch tensor with explicit dtype just in case\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float, device=self.device)\n",
    "        # model accepts non-batched input, proceed as is\n",
    "        action_distr = self.pi_distr(state_tensor)\n",
    "        return action_distr.sample().numpy()\n",
    "\n",
    "    \n",
    "    def fit(self, n_epochs=100, n_trajectories=20, max_length=200, lr=0.001, gamma=0.99, batch_size=128, eps_d=None, verbose=None):\n",
    "        \"\"\"\n",
    "        This algorithm performs PPO learning on a bunch (sized n_trajectories) of trajectories (w/ length <= max_length) for n_iterations\n",
    "            lr defines learning rate of built-in Adam optimizer\n",
    "            verbose>0 sets up a period of learning process rendering\n",
    "        NB: .fit internally uses .act method of child class(this), doesn't inherit parental\n",
    "        \"\"\"\n",
    "\n",
    "        self.V_model.to(self.device)\n",
    "        self.pi_model.to(self.device)\n",
    "        V_optimizer = torch.optim.Adam(self.V_model.parameters(), lr=lr)\n",
    "        pi_optimizer = torch.optim.Adam(self.pi_model.parameters(), lr=lr)\n",
    "        \n",
    "        epochs_pbar = tqdm.trange(n_epochs, position=0, leave=True, colour=\"#a2d2ff\")\n",
    "        dh = display.display(display_id=True)\n",
    "        # use linear annealing rule default or customize given one\n",
    "        if eps_d is not None:\n",
    "            eps_d.n_total = n_epochs\n",
    "        else:\n",
    "            eps_d = LinearAR(n_iterations=n_iterations, start=0.2)\n",
    "        self.log, eps_log, V_loss_log, pi_loss_log = [], [], [], []\n",
    "        for e in epochs_pbar:\n",
    "            # obtain epsilon\n",
    "            eps = eps_d(e)\n",
    "            # trace several routes in order to deal with stochastic environment\n",
    "            results = [self.walk_ga(max_length, gamma=gamma) for t in range(n_trajectories)]\n",
    "            # merge results into a big chunk\n",
    "            S, A, R_tot, G, Af = map(torch.cat, zip(*results)) # nb! default concatenation dim=0\n",
    "            a_lp = self.pi_distr(S).log_prob(A).detach()\n",
    "            # batch processing\n",
    "            curr_data = PPOIDataset(S, A, G, Af)\n",
    "            for (S_b, A_b, G_b, Af_b, idx) in DataLoader(curr_data, batch_size, shuffle=True):\n",
    "                # extract policy model probabilities for given actions\n",
    "                a_lp_curr = self.pi_distr(S_b).log_prob(A_b)\n",
    "                # numerically stable approach (probabilities might be too close to zero, underflow)\n",
    "                ratio = torch.exp(a_lp[idx] - a_lp_curr)\n",
    "                Af_b = Af_b.unsqueeze(-1).detach() # because Dataloader changes (squeezes) that\n",
    "                # Af_b = G_b.detach() - self.V_model(S_b)\n",
    "                # forward pass: maximize this quantity (set up as loss) and minimize difference between returns and model of V\n",
    "                pi_loss = - torch.mean(torch.minimum(ratio * Af_b, torch.clamp(ratio, 1 - eps, 1 + eps) * Af_b))\n",
    "                V_loss = self.loss(self.V_model(S_b), G_b.detach())\n",
    "                # double backward pass\n",
    "                V_optimizer.zero_grad()\n",
    "                pi_optimizer.zero_grad()\n",
    "                V_loss.backward()\n",
    "                pi_loss.backward()\n",
    "                V_optimizer.step()\n",
    "                pi_optimizer.step()\n",
    "\n",
    "            # logging\n",
    "            eps_log.append(eps)\n",
    "            V_loss_log.append(V_loss.detach().item())\n",
    "            pi_loss_log.append(pi_loss.detach().item())\n",
    "            self.log.append(torch.mean(R_tot).item())\n",
    "            epochs_pbar.set_postfix_str(f'curr reward: {self.log[-1]:.1f} | V_loss: {V_loss_log[-1]:.2f} | pi_loss: {pi_loss_log[-1]:.2f}', refresh=True)\n",
    "            # visualization (plotting starts after at least 1 iteration)\n",
    "            if verbose and e > 0 and (e + 1) % verbose == 0:\n",
    "                ax = self.learning_curve(title=\"Rewards\")\n",
    "                ax2 = ax.twinx()\n",
    "                ax2.tick_params(axis='y', labelcolor='slateblue')\n",
    "                # sns.lineplot(eps_log, linewidth=0.5, ax=ax2, label=\"exploration, ε\", color='slateblue')\n",
    "                ax2 = sns.lineplot(V_loss_log, linewidth=0.5, label='V loss', ax=ax2, color='coral')\n",
    "                sns.lineplot(pi_loss_log, linewidth=0.5, label='π loss', ax=ax2, color='palevioletred')\n",
    "                dh.update(plt.gcf())\n",
    "                plt.close()  # because plt.clf() is spurious\n",
    "        return self.log[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c751d02b-d887-4f8b-b357-543033e19240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf39bd7f289481cb71b6785d8a6ec42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FUGG\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (64, 1)) of distribution Normal(loc: torch.Size([64, 1]), scale: torch.Size([64, 1])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan]], grad_fn=<ClampBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m ppo \u001b[38;5;241m=\u001b[39m PPOAgent(env\u001b[38;5;241m=\u001b[39mENV)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trajectories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps_d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAnnealingRate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[27], line 117\u001b[0m, in \u001b[0;36mPPOAgent.fit\u001b[1;34m(self, n_epochs, n_trajectories, max_length, lr, gamma, batch_size, eps_d, verbose)\u001b[0m\n\u001b[0;32m    114\u001b[0m curr_data \u001b[38;5;241m=\u001b[39m PPOIDataset(S, A, G, Af)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (S_b, A_b, G_b, Af_b, idx) \u001b[38;5;129;01min\u001b[39;00m DataLoader(curr_data, batch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# extract policy model probabilities for given actions\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     a_lp_curr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpi_distr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mS_b\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlog_prob(A_b)\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# numerically stable approach (probabilities might be too close to zero, underflow)\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     ratio \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(a_lp[idx] \u001b[38;5;241m-\u001b[39m a_lp_curr)\n",
      "Cell \u001b[1;32mIn[27], line 58\u001b[0m, in \u001b[0;36mPPOAgent.pi_distr\u001b[1;34m(self, states_tensor)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(mean_raw)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFUGG\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\DRL_1\\venv\\Lib\\site-packages\\torch\\distributions\\normal.py:56\u001b[0m, in \u001b[0;36mNormal.__init__\u001b[1;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m     batch_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\DRL_1\\venv\\Lib\\site-packages\\torch\\distributions\\distribution.py:68\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     66\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m---> 68\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     69\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m             )\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (64, 1)) of distribution Normal(loc: torch.Size([64, 1]), scale: torch.Size([64, 1])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan],\n        [nan]], grad_fn=<ClampBackward0>)"
     ]
    }
   ],
   "source": [
    "ppo = PPOAgent(env=ENV)\n",
    "ppo.fit(n_epochs=40, n_trajectories=20, max_length=200, lr=1e-6, eps_d=AnnealingRate(start=0.2), gamma=0.99, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e05312-eda9-43ea-b064-5d1cc490a382",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
