{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8793010-268a-45bf-856e-0751e32ef802",
   "metadata": {},
   "outputs": [],
   "source": [
    "## !pip3 install -q -r requirements.txt\n",
    "# !pipreqsnb --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "3f1a1593-34aa-4b90-9949-69c570debeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import seaborn as sns\n",
    "from IPython import display\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "from torch.distributions import Normal\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "c054b98d-7cf0-4940-ace3-1581e8954043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import ModelFreeAgent, CRandAgent\n",
    "from aux_func import LinearAR, ExponentialAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "9773d1da-3810-4b77-ba9c-ad43ed3e0bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = gym.make(\"LunarLanderContinuous-v2\", render_mode=\"rgb_array\")\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# Pendulum-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "3441d062-2af1-4d85-8007-178a20ab51f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOIDataset(Dataset):\n",
    "    '''intermediate structure for PyTorch DataLoader'''\n",
    "    def __init__(self, states, actions, returns, advantage_values):\n",
    "        self.S = states\n",
    "        self.A = actions\n",
    "        self.Af = advantage_values\n",
    "        self.R = returns\n",
    "        assert len(self.S) == len(self.A) == len(self.Af) == len(self.R), 'wrong input'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.S)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.S[i], self.A[i], self.Af[i], self.R[i], i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "id": "014c9dbf-55c1-4f14-9a78-d8f085768dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent(ModelFreeAgent, CRandAgent):\n",
    "    def __init__(self, env, aid_to_str=None, hidden_d=(64, 32), device=DEVICE):\n",
    "        super().__init__(env=env, aid_to_str=aid_to_str)\n",
    "        self.d_actions = env.unwrapped.action_space.shape[0]\n",
    "        self.a_ll = torch.Tensor(env.action_space.low)\n",
    "        self.a_ul = torch.Tensor(env.action_space.high)\n",
    "        \n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = device\n",
    "        self.frozen_pi = None\n",
    "        self.z_eps = 1E-3 # for internal zero-division issues\n",
    "        \n",
    "        # model for a value function (scalar)\n",
    "        self.V_model = nn.Sequential(\n",
    "            nn.Linear(in_features=self.d_states, out_features=hidden_d[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_d[0], out_features=hidden_d[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_d[1], 1))\n",
    "        \n",
    "        # predicts parameters mu, std of action distribution (implying that's a Normal d.)\n",
    "        self.pi_model = nn.Sequential(\n",
    "            nn.Linear(in_features=self.d_states, out_features=hidden_d[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_d[0], out_features=hidden_d[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_d[1], 2 * self.d_actions),\n",
    "            # nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # on-the-fly advantage values calculation\n",
    "        self.fly = self.adv\n",
    "        self.Af = None\n",
    "        \n",
    "    def adv(self, queue_6, terminated, last_state, gamma):\n",
    "        \"\"\"Calculates advantage function online, throughout current trajectory,\n",
    "            each tensor allows backprop gradient flow\"\"\"\n",
    "        if len(queue_6) == 6:  # sarsar\n",
    "            # cast to torch tensors on device with float dtype (evades longint inheritance)\n",
    "            s, a, r, sx, ax, rx = map(lambda x: torch.tensor(x, dtype=torch.float, device=self.device), list(queue_6))\n",
    "            self.Af.append(r + gamma * self.V_model(sx).detach() - self.V_model(s))\n",
    "            # additional xxx|sar + terminal state as next action doesn't happen\n",
    "            if terminated: # done or exhausted\n",
    "                s, a, r, sx = sx, ax, rx, torch.tensor(last_state, dtype=torch.float, device=self.device)\n",
    "                self.Af.append(r + gamma * self.V_model(sx).detach() - self.V_model(s))\n",
    "                \n",
    "\n",
    "    def pi_distr(self, states_tensor):\n",
    "        \"\"\"transforms net output of current policy model to parameters of distribution on actions at these states (as multidimensional continuous RV)\n",
    "        torch-compliant method, allows backprop gradient flow\"\"\"\n",
    "        # split net output on two parts\n",
    "        mean_raw, std_raw = torch.split(self.pi_model(states_tensor), 2, dim=-1)\n",
    "        # ensure that they do obey limits (as parameters of Normal distribution)\n",
    "        mean = torch.clamp(mean_raw, self.a_ll + self.z_eps ** 2, self.a_ul - self.z_eps ** 2) # clipping for a given action interval\n",
    "        std = nn.functional.threshold(std_raw, threshold=0, value=self.z_eps) # prevent non-positive std\n",
    "        return Normal(mean, std)\n",
    "        \n",
    "        \n",
    "    def walk_ga(self, max_length, gamma):\n",
    "        \"\"\"transforms standard results after tracing a route, yields\n",
    "            standard SA as numpy arrays, total reward, then G-returns and Advantage value tensors\n",
    "            only Af values have .grad attribute\"\"\"\n",
    "        self.Af = []\n",
    "        trajectory = super().walk(max_length=max_length, gamma=gamma)\n",
    "        S, A, R = trajectory['s'], trajectory['a'], trajectory['r']\n",
    "        #get rewards serie (summands, w/ discounting)\n",
    "        gamma_ = np.cumprod(np.concatenate((np.atleast_1d(1.), np.tile(np.array(gamma), len(R) - 1))))\n",
    "        R_discounted = gamma_ * np.array(R)\n",
    "        G = np.array([np.sum(R_discounted[t:]) / gamma ** t for t in range(len(R_discounted))])\n",
    "        return torch.Tensor(np.array(S)), torch.Tensor(np.array(A)), torch.sum(torch.Tensor(R)).unsqueeze(0), torch.tensor(G, dtype=torch.float), torch.stack(self.Af)\n",
    "        \n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"samples an action from a normal distribution (mean, std are given by NN)\"\"\"\n",
    "        # convert to torch tensor with explicit dtype just in case\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float, device=self.device)\n",
    "        # model accepts non-batched input, proceed as is\n",
    "        action_distr = self.pi_distr(state_tensor)\n",
    "        return action_distr.sample().numpy()\n",
    "\n",
    "    \n",
    "    def fit(self, n_iterations=100, n_trajectories=20, max_length=200, lr=0.001, gamma=0.99, batch_size=128, eps_d=None, verbose=None):\n",
    "        \"\"\"\n",
    "        This algorithm performs PPO learning on a bunch (sized n_trajectories) of trajectories (w/ length <= max_length) for n_iterations\n",
    "            lr defines learning rate of built-in Adam optimizer\n",
    "            verbose>0 sets up a period of learning process rendering\n",
    "        NB: .fit internally uses .act method of child class(this), doesn't inherit parental\n",
    "        \"\"\"\n",
    "\n",
    "        self.V_model.to(self.device)\n",
    "        self.pi_model.to(self.device)\n",
    "        V_optimizer = torch.optim.Adam(self.V_model.parameters(), lr=lr)\n",
    "        pi_optimizer = torch.optim.Adam(self.pi_model.parameters(), lr=lr)\n",
    "        \n",
    "        iterations_pbar = tqdm.trange(n_iterations, position=0, leave=True, colour=\"#a2d2ff\")\n",
    "        dh = display.display(display_id=True)\n",
    "        # use linear annealing rule default or customize given one\n",
    "        if eps_d is not None:\n",
    "            eps_d.n_total = n_trajectories\n",
    "        else:\n",
    "            eps_d = LinearAR(n_iterations=n_iterations, start=0.2)\n",
    "\n",
    "        self.log, eps_log, V_loss_log, pi_loss_log = [], [], [], []\n",
    "        for i in iterations_pbar:\n",
    "            # obtain epsilon\n",
    "            eps = eps_d(i)\n",
    "            # trace several routes in order to deal with stochastic environment\n",
    "            results = [self.walk_ga(max_length, gamma=gamma) for t in range(n_trajectories)]\n",
    "            # merge results into a big chunk\n",
    "            S, A, R_tot, G, Af = map(torch.cat, zip(*results)) # nb! default concatenation dim=0\n",
    "            a_lp = self.pi_distr(S).log_prob(A).detach()\n",
    "            # batch processing\n",
    "            curr_data = PPOIDataset(S, A, G, Af)\n",
    "            for (S_b, A_b, G_b, Af_b, idx) in DataLoader(curr_data, batch_size, shuffle=True):\n",
    "                # extract policy model probabilities for given actions\n",
    "                a_lp_curr = self.pi_distr(S_b).log_prob(A_b)\n",
    "                # numerically stable approach (probabilities might be too close to zero, underflow)\n",
    "                ratio = torch.exp(a_lp_curr - a_lp[idx])\n",
    "                Af_b = Af_b.unsqueeze(-1).detach() # because Dataloader changes (squeezes) that\n",
    "                # forward pass: maximize this quantity (set up as loss) and minimize difference between returns and model of V\n",
    "                pi_loss = - torch.mean(torch.minimum(ratio * Af_b, torch.clamp(ratio, 1 - eps, 1 + eps) * Af_b))\n",
    "                V_loss = self.loss(self.V_model(S_b), G_b.detach())\n",
    "                # double backward pass \n",
    "                V_optimizer.zero_grad()\n",
    "                pi_optimizer.zero_grad()\n",
    "                V_loss.backward()\n",
    "                pi_loss.backward()\n",
    "                V_optimizer.step()\n",
    "                pi_optimizer.step()\n",
    "\n",
    "            # logging\n",
    "            eps_log.append(eps)\n",
    "            V_loss_log.append(V_loss.detach().item())\n",
    "            pi_loss_log.append(pi_loss.detach().item())\n",
    "            self.log.append(torch.mean(R_tot).item())\n",
    "            iterations_pbar.set_postfix_str(f'curr reward: {self.log[-1]:.1f} | V_loss: {V_loss_log[-1]:.2f} | pi_loss: {pi_loss_log[-1]:.2f}', refresh=True)\n",
    "            # visualization (plotting starts after at least 1 iteration)\n",
    "            if verbose and i > 0 and (i + 1) % verbose == 0:\n",
    "                ax = self.learning_curve(title=\"Rewards\")\n",
    "                ax2 = ax.twinx()\n",
    "                ax2.tick_params(axis='y', labelcolor='slateblue')\n",
    "                # sns.lineplot(eps_log, linewidth=0.5, ax=ax2, label=\"exploration, ε\", color='slateblue')\n",
    "                ax2 = sns.lineplot(V_loss_log, linewidth=0.5, label='V loss', ax=ax2, color='coral')\n",
    "                sns.lineplot(pi_loss_log, linewidth=0.5, label='π loss', ax=ax2, color='palevioletred')\n",
    "                dh.update(plt.gcf())\n",
    "                plt.close()  # because plt.clf() is spurious\n",
    "        return self.log[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "id": "c751d02b-d887-4f8b-b357-543033e19240",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo = PPOAgent(env=ENV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b233613d-6f96-4da7-819a-3877a4e193c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "575313627e9a4a14a0feb52c60d425af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ppo.fit(n_iterations=3, n_trajectories=20, max_length=200, lr=1e-3, gamma=0.99, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "id": "00062377-03ed-4ec2-8066-71d2d5c29327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-24.091156005859375, -15.394144058227539, -11.851909637451172]"
      ]
     },
     "execution_count": 747,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5e3d6c-8851-4777-8517-a29788063dae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a13e50-00e5-4d0d-86d6-d95793aea51b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf19a60-0901-4000-8177-5ebcde37cc64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
